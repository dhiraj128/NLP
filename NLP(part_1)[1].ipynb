{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e95ff301-7f5f-4fd4-89e9-3767c68912c5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f35c48f3-17f3-463b-a73c-e42dab7aff4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: rahul wakes early day goes college morning comes 3 pm present rahul outside buy snacks\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Tokenize and clean text\n",
    "    doc = nlp(text)\n",
    "    cleaned_tokens = [token.text.lower() for token in doc if not token.is_stop and not token.is_punct]\n",
    "    return \" \".join(cleaned_tokens), doc\n",
    "\n",
    "# Example text\n",
    "text = \"Rahul wakes up early every day. He goes to college in the morning and comes back at 3 pm. At present, Rahul is outside. He has to buy the snacks for all of us.\"\n",
    "\n",
    "# Preprocess text\n",
    "cleaned_text, doc = preprocess_text(text)\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6953ec1e-8a34-4ab3-ba39-216705486751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identified Tasks: [{'action': 'buy', 'who': None, 'deadline': None}]\n"
     ]
    }
   ],
   "source": [
    "def identify_tasks(doc):\n",
    "    tasks = []\n",
    "    for sent in doc.sents:\n",
    "        for token in sent:\n",
    "            # Check for actionable verbs\n",
    "            if token.pos_ == \"VERB\" and token.lemma_ in [\"buy\", \"clean\", \"review\", \"schedule\"]:  # Add more verbs\n",
    "                task = {\"action\": token.lemma_, \"who\": None, \"deadline\": None}\n",
    "                # Extract subject (who)\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == \"nsubj\":\n",
    "                        task[\"who\"] = child.text\n",
    "                # Extract deadline\n",
    "                for child in token.children:\n",
    "                    if child.dep_ == \"prep\" and child.text == \"by\":\n",
    "                        task[\"deadline\"] = next(child.rights).text\n",
    "                tasks.append(task)\n",
    "    return tasks\n",
    "\n",
    "# Identify tasks\n",
    "tasks = identify_tasks(doc)\n",
    "print(\"Identified Tasks:\", tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a154cd4c-4b3a-4a6d-9b9b-cc1eb1297bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Categories: [[0.33333333 0.33333333 0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "def categorize_tasks(task_texts):\n",
    "    # Convert task texts into a matrix of token counts\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(task_texts)\n",
    "    \n",
    "    # Apply Latent Dirichlet Allocation (LDA)\n",
    "    lda = LatentDirichletAllocation(n_components=3, random_state=42)  # Adjust n_components as needed\n",
    "    lda.fit(X)\n",
    "    \n",
    "    # Assign categories to tasks\n",
    "    categories = lda.transform(X)\n",
    "    return categories\n",
    "\n",
    "# Extract task texts\n",
    "task_texts = [task[\"action\"] for task in tasks]\n",
    "\n",
    "# Categorize tasks\n",
    "categories = categorize_tasks(task_texts)\n",
    "print(\"Task Categories:\", categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "53fdabab-4576-455a-8acd-1ec7ff64d959",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Structured Output: [{'task': 'buy', 'who': None, 'deadline': None, 'category': 0}]\n"
     ]
    }
   ],
   "source": [
    "def generate_output(tasks, categories):\n",
    "    output = []\n",
    "    for task, category in zip(tasks, categories):\n",
    "        output.append({\n",
    "            \"task\": task[\"action\"],\n",
    "            \"who\": task[\"who\"],\n",
    "            \"deadline\": task[\"deadline\"],\n",
    "            \"category\": category.argmax()  # Assign the most probable category\n",
    "        })\n",
    "    return output\n",
    "\n",
    "# Generate output\n",
    "output = generate_output(tasks, categories)\n",
    "print(\"Structured Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea63c002-5675-47e6-97ea-4e7ac6246507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Output: [{'task': 'clean', 'who': None, 'deadline': 'pm', 'category': 2}, {'task': 'review', 'who': None, 'deadline': 'tomorrow', 'category': 1}]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"John needs to clean the room by 5 pm today. Sarah has to review the report by tomorrow.\"\n",
    "cleaned_text, doc = preprocess_text(test_text)\n",
    "tasks = identify_tasks(doc)\n",
    "task_texts = [task[\"action\"] for task in tasks]\n",
    "categories = categorize_tasks(task_texts)\n",
    "output = generate_output(tasks, categories)\n",
    "print(\"Test Output:\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcad1d8-c75f-4c89-86f5-c1291dbe98f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16f36aa-d1de-4d7f-8b28-660c9b4c76fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
